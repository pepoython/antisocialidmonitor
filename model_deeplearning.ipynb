{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tugas Akhir Sistem Intelijen EL5004-02\n",
    "\n",
    "Fitri Andri Astuti/ 23220030\n",
    "\n",
    "Berikut merupakan salah satu bagian code dari fungsi data_classification pada dokumentasi API sistem layanan monitoring perilaku antisosial publik Twitter Indonesia dapat dilihat pada https://antisocialinadev.herokuapp.com/docs dan frontend https://antisocialinadev.herokuapp.com/ dan https://antisocialinamonitor.herokuapp.com/.\n",
    "\n",
    "Dataset yang dikumpulkan berdasarkan keyword pada kelima class perilaku antisocial yang didapat dari crawling Twitter API v2. Data tersebut, penulis sudah membuat dataset tersebut pada platform Kaggle https://www.kaggle.com/fitriandri/antisocial-behaviour-public-twitter-indonesia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Unable to import required dependencies:\nnumpy: \n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\n\nWe have compiled some common reasons and troubleshooting tips at:\n\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n\nPlease note and check the following:\n\n  * The Python version is: Python3.9 from \"/Users/fitriandriastuti/opt/anaconda3/envs/ml/bin/python\"\n  * The NumPy version is: \"1.20.3\"\n\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\n\nOriginal error was: dlopen(/Users/fitriandriastuti/opt/anaconda3/envs/ml/lib/python3.9/site-packages/numpy/core/_multiarray_umath.cpython-39-darwin.so, 2): Library not loaded: @rpath/libopenblas.dylib\n  Referenced from: /Users/fitriandriastuti/opt/anaconda3/envs/ml/lib/python3.9/site-packages/numpy/core/_multiarray_umath.cpython-39-darwin.so\n  Reason: image not found\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-4-16f807b7aaef>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0;32mimport\u001B[0m \u001B[0mpandas\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mmatplotlib\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpyplot\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mplt\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mnumpy\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;31m# import warnings\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;31m# warnings.filterwarnings('ignore')  # \"error\", \"ignore\", \"always\", \"default\", \"module\" or \"once\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/ml/lib/python3.9/site-packages/pandas/__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mmissing_dependencies\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 16\u001B[0;31m     raise ImportError(\n\u001B[0m\u001B[1;32m     17\u001B[0m         \u001B[0;34m\"Unable to import required dependencies:\\n\"\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m\"\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmissing_dependencies\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     18\u001B[0m     )\n",
      "\u001B[0;31mImportError\u001B[0m: Unable to import required dependencies:\nnumpy: \n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\n\nWe have compiled some common reasons and troubleshooting tips at:\n\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n\nPlease note and check the following:\n\n  * The Python version is: Python3.9 from \"/Users/fitriandriastuti/opt/anaconda3/envs/ml/bin/python\"\n  * The NumPy version is: \"1.20.3\"\n\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\n\nOriginal error was: dlopen(/Users/fitriandriastuti/opt/anaconda3/envs/ml/lib/python3.9/site-packages/numpy/core/_multiarray_umath.cpython-39-darwin.so, 2): Library not loaded: @rpath/libopenblas.dylib\n  Referenced from: /Users/fitriandriastuti/opt/anaconda3/envs/ml/lib/python3.9/site-packages/numpy/core/_multiarray_umath.cpython-39-darwin.so\n  Reason: image not found\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')  # \"error\", \"ignore\", \"always\", \"default\", \"module\" or \"once\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Unable to import required dependencies:\nnumpy: \n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\n\nWe have compiled some common reasons and troubleshooting tips at:\n\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n\nPlease note and check the following:\n\n  * The Python version is: Python3.9 from \"/Users/fitriandriastuti/opt/anaconda3/envs/ml/bin/python\"\n  * The NumPy version is: \"1.20.3\"\n\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\n\nOriginal error was: dlopen(/Users/fitriandriastuti/opt/anaconda3/envs/ml/lib/python3.9/site-packages/numpy/core/_multiarray_umath.cpython-39-darwin.so, 2): Library not loaded: @rpath/libopenblas.dylib\n  Referenced from: /Users/fitriandriastuti/opt/anaconda3/envs/ml/lib/python3.9/site-packages/numpy/core/_multiarray_umath.cpython-39-darwin.so\n  Reason: image not found\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-2-b32d84402a6a>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0;32mimport\u001B[0m \u001B[0mpandas\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;31m# data = pd.read_csv(dataset)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread_excel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mr'tweet_train_two_class.xlsx'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msheet_name\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'tweet_train_two_class'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhead\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/ml/lib/python3.9/site-packages/pandas/__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mmissing_dependencies\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 16\u001B[0;31m     raise ImportError(\n\u001B[0m\u001B[1;32m     17\u001B[0m         \u001B[0;34m\"Unable to import required dependencies:\\n\"\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m\"\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmissing_dependencies\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     18\u001B[0m     )\n",
      "\u001B[0;31mImportError\u001B[0m: Unable to import required dependencies:\nnumpy: \n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\n\nWe have compiled some common reasons and troubleshooting tips at:\n\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n\nPlease note and check the following:\n\n  * The Python version is: Python3.9 from \"/Users/fitriandriastuti/opt/anaconda3/envs/ml/bin/python\"\n  * The NumPy version is: \"1.20.3\"\n\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\n\nOriginal error was: dlopen(/Users/fitriandriastuti/opt/anaconda3/envs/ml/lib/python3.9/site-packages/numpy/core/_multiarray_umath.cpython-39-darwin.so, 2): Library not loaded: @rpath/libopenblas.dylib\n  Referenced from: /Users/fitriandriastuti/opt/anaconda3/envs/ml/lib/python3.9/site-packages/numpy/core/_multiarray_umath.cpython-39-darwin.so\n  Reason: image not found\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# data = pd.read_csv(dataset)\n",
    "data = pd.read_excel(r'tweet_train_two_class.xlsx', sheet_name='tweet_train_two_class')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data.Label = data.idClass\n",
    "data.Label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data['Label_Numeric'] = data['idClass']\n",
    "data.Label_Numeric.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "stop = stopwords.words('english')\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data['Tweet'] = data['formalizing']\n",
    "data['AfterStopwords'] = data['Tweet'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "data['AfterPunctuation'] = data['AfterStopwords'].apply(lambda x:''.join([i for i in x\n",
    "                                                  if i not in string.punctuation]))\n",
    "\n",
    "data['cleaned'] = data['AfterPunctuation'].str.lower()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n",
    "\n",
    "data['lemmatized'] = data['cleaned'].apply(lemmatize_text)\n",
    "\n",
    "\n",
    "data[\"final1\"]= data[\"lemmatized\"].str.join(\" \")\n",
    "\n",
    "data['final1'] = data['final1'].str.replace('\\d+', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data['final1'][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# %tensorflow_version 1.x\n",
    "\n",
    "# from keras import backend as K\n",
    "# K.tensorflow_backend._get_available_gpus()\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "maxlen = 280\n",
    "max_words = 40000\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(data['Tweet'])\n",
    "sequences = tokenizer.texts_to_sequences(data['Tweet'])\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "data1 = pad_sequences(sequences, maxlen=maxlen)\n",
    "print(data1)\n",
    "def to_one_hot(labels, dimension=6):\n",
    "    results = np.zeros((len(labels), dimension))\n",
    "    for i, label in enumerate(labels):\n",
    "        results[i, label] = 1.\n",
    "    return results\n",
    "one_hot_labels = to_one_hot(data['Label_Numeric'])\n",
    "labels = np.asarray(one_hot_labels)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "indices = np.arange(data1.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data1 = data1[indices]\n",
    "labels = labels[indices]\n",
    "\"\"\"x_train = data1[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "x_val = data1[training_samples: training_samples + validation_samples]\n",
    "y_val = labels[training_samples: training_samples + validation_samples]\n",
    "x_test = data1[validation_samples: validation_samples + test_samples]\n",
    "y_test = labels[validation_samples: validation_samples + test_samples]\"\"\"\n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data1, labels, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(labels.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "model_Word2Vec = gensim.models.Word2Vec(data, min_count = 1, vector_size = 100, window = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.layers import Embedding, LSTM\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from tensorflow.python.ops.math_ops import reduce_prod\n",
    "\n",
    "embedding_dim = 300\n",
    "\n",
    "model3 = Sequential()\n",
    "model3.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "model3.add(Bidirectional(layers.LSTM(32,return_sequences = True, dropout=0.2,recurrent_dropout=0.2)))\n",
    "model3.add(Bidirectional(layers.LSTM(32, return_sequences= True)))\n",
    "model3.add(layers.GlobalMaxPooling1D())\n",
    "model3.add(layers.Dense(10, activation='relu'))\n",
    "model3.add(layers.Dense(6, activation='softmax'))\n",
    "model3.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model3.summary()\n",
    "\n",
    "\n",
    "\n",
    "history = model3.fit(X_train, y_train, epochs=10, batch_size=2048, validation_split = 0.1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score , recall_score\n",
    "y_val_pred=model3.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "rounded_labels=np.argmax(y_test, axis=1)\n",
    "\n",
    "\n",
    "# accuracy: (tp + tn) / (p + n)\n",
    "accuracy = accuracy_score(rounded_labels, y_val_pred)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "# precision tp / (tp + fp)\n",
    "precision = precision_score(rounded_labels, y_val_pred, average = 'micro')\n",
    "print('Precision: %f' %precision)\n",
    "# recall: tp / (tp + fn)\n",
    "recall = recall_score(rounded_labels, y_val_pred, average = 'micro')\n",
    "print('Recall: %f' %recall)\n",
    "# f1: 2 tp / (2 tp + fp + fn)\n",
    "f1 = f1_score(rounded_labels, y_val_pred, average = 'micro')\n",
    "print('F1: %f' %f1)\n",
    "# accuracy: (tp + tn) / (p + n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print('LSTM: ')\n",
    "# print(confusion_matrix(rounded_labels, y_val_pred))\n",
    "print(classification_report(rounded_labels, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}